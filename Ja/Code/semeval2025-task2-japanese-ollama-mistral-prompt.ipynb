{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10715587,"sourceType":"datasetVersion","datasetId":6347856}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n    # for filename in filenames:\n    #     print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:18:13.921087Z","iopub.execute_input":"2025-02-10T19:18:13.921333Z","iopub.status.idle":"2025-02-10T19:18:14.907521Z","shell.execute_reply.started":"2025-02-10T19:18:13.921309Z","shell.execute_reply":"2025-02-10T19:18:14.906737Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip -q install --upgrade ollama","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:18:19.224752Z","iopub.execute_input":"2025-02-10T19:18:19.225057Z","iopub.status.idle":"2025-02-10T19:18:24.324446Z","shell.execute_reply.started":"2025-02-10T19:18:19.225034Z","shell.execute_reply":"2025-02-10T19:18:24.323012Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:18:24.326056Z","iopub.execute_input":"2025-02-10T19:18:24.326446Z","iopub.status.idle":"2025-02-10T19:19:18.312261Z","shell.execute_reply.started":"2025-02-10T19:18:24.326413Z","shell.execute_reply":"2025-02-10T19:19:18.311243Z"}},"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n############################################################################################# 100.0%                                                                                      1.1%                                                                           2.5%############                                                                            21.4%###############                                                                      27.6%##################                                                             37.1%####################################                                                   47.6%################################################                        76.4%#########################################################                       78.3%###############################################################                   82.6%##############################################################               86.4%################################            89.5%##########################################################################           90.5%#######################################################################         92.8%#############################################################      95.9%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nget_ipython().system = os.system","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:19:18.313939Z","iopub.execute_input":"2025-02-10T19:19:18.314246Z","iopub.status.idle":"2025-02-10T19:19:18.318310Z","shell.execute_reply.started":"2025-02-10T19:19:18.314221Z","shell.execute_reply":"2025-02-10T19:19:18.317509Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!ollama serve &","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:19:18.319670Z","iopub.execute_input":"2025-02-10T19:19:18.319918Z","iopub.status.idle":"2025-02-10T19:19:18.337652Z","shell.execute_reply.started":"2025-02-10T19:19:18.319897Z","shell.execute_reply":"2025-02-10T19:19:18.336618Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"!ollama pull mistral","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:19:18.338986Z","iopub.execute_input":"2025-02-10T19:19:18.339330Z","iopub.status.idle":"2025-02-10T19:19:53.575330Z","shell.execute_reply.started":"2025-02-10T19:19:18.339297Z","shell.execute_reply":"2025-02-10T19:19:53.574337Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\nimport ollama\n\n# from langchain_community.chat_models import ChatOllama\n# from langchain.document_loaders import WikipediaLoader\n# # from langchain_community.llms import Ollama\n# from langchain_ollama import OllamaLLM\n# from langchain.chains import LLMChain\n# from langchain.prompts.chat import (ChatPromptTemplate,HumanMessagePromptTemplate,SystemMessagePromptTemplate)\n# from langchain import PromptTemplate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:19:53.576535Z","iopub.execute_input":"2025-02-10T19:19:53.576895Z","iopub.status.idle":"2025-02-10T19:19:54.574085Z","shell.execute_reply.started":"2025-02-10T19:19:53.576858Z","shell.execute_reply":"2025-02-10T19:19:54.573129Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Path to your input and output files\ninput_file = \"/kaggle/input/task-2-ea-mt-entity-aware-machine-translation/ja_JP_after_c51833f9c3baafbf.jsonl\"\noutput_file = \"translated_data_en_ja_ollama-mistral-after-c51833f9c3baafbf-v1.jsonl\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:19:54.576511Z","iopub.execute_input":"2025-02-10T19:19:54.576782Z","iopub.status.idle":"2025-02-10T19:19:54.580859Z","shell.execute_reply.started":"2025-02-10T19:19:54.576760Z","shell.execute_reply":"2025-02-10T19:19:54.579808Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def translate_text(obj):\n    prompt = f\"\"\"\n    Translate the following English text into Japanese with a more attention on accurately translating the given name entities. \n    Do not add extra text.\n    \n    Entity Types: {', '.join(obj['entity_types'])}\n    Text: \"{obj['source']}\"\n    \n    Output the result in JSON format:\n    {{\n        \"id\": \"{obj['id']}\",\n        \"predicted_text\": \"<translated_text>\"\n    }}\n    \"\"\"\n    \n    response = ollama.chat(\n        model=\"mistral\",\n        messages=[{\"role\": \"system\", \"content\": \"You are a translation assistant.\"},\n                  {\"role\": \"user\", \"content\": prompt}],\n        stream=False\n    )\n    \n    # Extract generated text and parse JSON\n    try:\n        # print(response)\n        output_json = json.loads(response.message.content)\n        return output_json\n    except json.JSONDecodeError:\n        print(f\"Error parsing response for ID: {obj['id']}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:19:54.581869Z","iopub.execute_input":"2025-02-10T19:19:54.582196Z","iopub.status.idle":"2025-02-10T19:19:54.597436Z","shell.execute_reply.started":"2025-02-10T19:19:54.582157Z","shell.execute_reply":"2025-02-10T19:19:54.596345Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"with open(input_file, \"r\", encoding=\"utf-8\") as f:\n    total_lines = sum(1 for _ in f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:19:54.598720Z","iopub.execute_input":"2025-02-10T19:19:54.599041Z","iopub.status.idle":"2025-02-10T19:19:54.626842Z","shell.execute_reply.started":"2025-02-10T19:19:54.599014Z","shell.execute_reply":"2025-02-10T19:19:54.625803Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Read input file and translate each object\nwith open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n    with tqdm(total=total_lines, desc=\"Generating Predictions\", colour=\"blue\") as pbar:\n            for line in infile:\n                obj = json.loads(line.strip())\n                translated_obj = translate_text(obj)\n                # print(\"Before: \", translated_obj)\n                \n                # print(\"After: \", translated_obj)\n                # break\n                if translated_obj:\n                    translated_obj = {\n                    \"id\": translated_obj[\"id\"],\n                    \"source_language\": \"English\",\n                    \"target_language\": \"Japanese\", \n                    \"text\": obj[\"source\"],\n                    \"prediction\": translated_obj[\"predicted_text\"]\n                    \n                    }\n                    outfile.write(json.dumps(translated_obj, ensure_ascii=False) + \"\\n\")\n                pbar.update(1)\n\nprint(f\"Translation completed! Output saved to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T19:19:54.627753Z","iopub.execute_input":"2025-02-10T19:19:54.628008Z"}},"outputs":[{"name":"stderr","text":"Generating Predictions:   4%|\u001b[34m▍         \u001b[0m| 65/1676 [01:51<1:04:29,  2.40s/it]","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: b07799945ba485d3\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  14%|\u001b[34m█▍        \u001b[0m| 237/1676 [07:03<1:12:47,  3.03s/it]","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: ee1e1eff98cd120e\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  20%|\u001b[34m██        \u001b[0m| 340/1676 [10:10<43:39,  1.96s/it]  ","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: b05159c78a85360b\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  43%|\u001b[34m████▎     \u001b[0m| 718/1676 [21:16<33:43,  2.11s/it]","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: 137c1ab5fb9e2b2b\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  44%|\u001b[34m████▍     \u001b[0m| 740/1676 [21:55<30:39,  1.96s/it]","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: f0c4d42eb8a53bf3\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  48%|\u001b[34m████▊     \u001b[0m| 802/1676 [23:47<32:00,  2.20s/it]","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: 8f9c0d52cfdc4064\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  68%|\u001b[34m██████▊   \u001b[0m| 1132/1676 [33:33<19:15,  2.12s/it]","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: bb5f68c1034c1d05\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  68%|\u001b[34m██████▊   \u001b[0m| 1145/1676 [33:58<20:21,  2.30s/it]","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: 5789a457e6a63b36\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  76%|\u001b[34m███████▌  \u001b[0m| 1267/1676 [37:32<11:17,  1.66s/it]","output_type":"stream"},{"name":"stdout","text":"Error parsing response for ID: 6a75abcaf7013232\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions:  77%|\u001b[34m███████▋  \u001b[0m| 1297/1676 [38:24<11:06,  1.76s/it]","output_type":"stream"}],"execution_count":null}]}