{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10752560,"sourceType":"datasetVersion","datasetId":6347856}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:47:43.268261Z","iopub.execute_input":"2025-02-15T20:47:43.268626Z","iopub.status.idle":"2025-02-15T20:47:43.570929Z","shell.execute_reply.started":"2025-02-15T20:47:43.268594Z","shell.execute_reply":"2025-02-15T20:47:43.570016Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Sources\n- https://huggingface.co/google/mt5-base\n- https://medium.com/@tskumar1320/how-to-fine-tune-pre-trained-language-translation-model-3e8a6aace9f\n- https://www.geeksforgeeks.org/machine-translation-with-transformer-in-python/\n- https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb\n- https://thepythoncode.com/article/machine-translation-using-huggingface-transformers-in-python\n- https://blog.speechmatics.com/huggingface-translation-triton\n- https://github.com/christianversloot/machine-learning-articles/blob/main/easy-machine-translation-with-machine-learning-and-huggingface-transformers.md","metadata":{}},{"cell_type":"markdown","source":"# Install Libraries and Modules","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:47:46.251417Z","iopub.execute_input":"2025-02-15T20:47:46.251888Z","iopub.status.idle":"2025-02-15T20:47:46.255904Z","shell.execute_reply.started":"2025-02-15T20:47:46.251857Z","shell.execute_reply":"2025-02-15T20:47:46.255021Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## WANDB Config","metadata":{}},{"cell_type":"code","source":"!pip -q uninstall -y wandb\n!pip -q install wandb==0.17.5 \nimport wandb\n!wandb offline\n!wandb disabled\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:47:47.979169Z","iopub.execute_input":"2025-02-15T20:47:47.979485Z","iopub.status.idle":"2025-02-15T20:48:10.330592Z","shell.execute_reply.started":"2025-02-15T20:47:47.979457Z","shell.execute_reply":"2025-02-15T20:48:10.329453Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hW&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\nW&B disabled.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip -q install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:48:10.332005Z","iopub.execute_input":"2025-02-15T20:48:10.332276Z","iopub.status.idle":"2025-02-15T20:48:14.212153Z","shell.execute_reply.started":"2025-02-15T20:48:10.332241Z","shell.execute_reply":"2025-02-15T20:48:14.211313Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q datasets evaluate sentencepiece accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:48:14.213770Z","iopub.execute_input":"2025-02-15T20:48:14.214001Z","iopub.status.idle":"2025-02-15T20:48:17.808153Z","shell.execute_reply.started":"2025-02-15T20:48:14.213981Z","shell.execute_reply":"2025-02-15T20:48:17.807072Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip uninstall transformers -y\n!pip -q install transformers==4.45.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:48:17.809869Z","iopub.execute_input":"2025-02-15T20:48:17.810203Z","iopub.status.idle":"2025-02-15T20:48:30.337585Z","shell.execute_reply.started":"2025-02-15T20:48:17.810172Z","shell.execute_reply":"2025-02-15T20:48:30.336536Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.47.0\nUninstalling transformers-4.47.0:\n  Successfully uninstalled transformers-4.47.0\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers import EvalPrediction\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:48:30.338709Z","iopub.execute_input":"2025-02-15T20:48:30.339023Z","iopub.status.idle":"2025-02-15T20:48:50.429644Z","shell.execute_reply.started":"2025-02-15T20:48:30.338999Z","shell.execute_reply":"2025-02-15T20:48:50.428876Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, file_path, tokenizer, max_length=128):\n        \"\"\"\n        Initializes the TranslationDataset.\n\n        Args:\n            file_path (str): Path to the JSONL file containing the pre-flattened data.\n            tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use for encoding.\n            max_length (int): The maximum length for tokenized sequences.\n        \"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            self.data = [json.loads(line) for line in f]\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        \"\"\"Returns the number of samples in the dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves the tokenized inputs and labels for a given index.\n\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            dict: A dictionary containing input IDs, attention mask, and labels.\n        \"\"\"\n        item = self.data[idx]\n        source = item['source']\n        target = item['target']\n\n        # Tokenize the source text\n        inputs = self.tokenizer(\n            source,\n            max_length=self.max_length,\n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )\n\n        # Tokenize the target text and prepare labels\n        labels = self.tokenizer(\n            target,\n            max_length=self.max_length,\n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )['input_ids']\n        # Replace padding token IDs with -100 for loss computation\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n            'labels': labels.squeeze()\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:48:50.430484Z","iopub.execute_input":"2025-02-15T20:48:50.431174Z","iopub.status.idle":"2025-02-15T20:48:50.437904Z","shell.execute_reply.started":"2025-02-15T20:48:50.431139Z","shell.execute_reply":"2025-02-15T20:48:50.436745Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\n\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,  \n    r=16, \n    lora_alpha=64,  \n    lora_dropout=0.1,  \n    bias=\"none\",\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n        \"fc1\",\n        \"fc2\"\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:48:50.438645Z","iopub.execute_input":"2025-02-15T20:48:50.438907Z","iopub.status.idle":"2025-02-15T20:48:50.457200Z","shell.execute_reply.started":"2025-02-15T20:48:50.438887Z","shell.execute_reply":"2025-02-15T20:48:50.456395Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Step 3: Load Pre-trained Model and Tokenizer\nmodel_name = \"Helsinki-NLP/opus-mt-en-jap\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:48:50.459289Z","iopub.execute_input":"2025-02-15T20:48:50.459474Z","iopub.status.idle":"2025-02-15T20:48:59.987171Z","shell.execute_reply.started":"2025-02-15T20:48:50.459458Z","shell.execute_reply":"2025-02-15T20:48:59.986243Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8d67697fa1d4d919f56c5841af54ad1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cae437a11574e0a91084ff331723eec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/509k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0a531f6d5dc4d72bed0d59229b55c5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59f25c1ade6e41a29e83892a81bba760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.64M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f554558dabb4c9f84867a9e468497a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/274M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b837d1d8e344809237f8680be469f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7466a023d24a4129ba8b0bbd8d0a0c2a"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:03:42.740356Z","iopub.execute_input":"2025-02-15T20:03:42.740588Z","iopub.status.idle":"2025-02-15T20:03:42.746048Z","shell.execute_reply.started":"2025-02-15T20:03:42.740568Z","shell.execute_reply":"2025-02-15T20:03:42.745242Z"}},"outputs":[{"name":"stdout","text":"MarianMTModel(\n  (model): MarianModel(\n    (shared): Embedding(80035, 512, padding_idx=80034)\n    (encoder): MarianEncoder(\n      (embed_tokens): Embedding(80035, 512, padding_idx=80034)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0-5): 6 x MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLU()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): MarianDecoder(\n      (embed_tokens): Embedding(80035, 512, padding_idx=80034)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0-5): 6 x MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLU()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=80035, bias=False)\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)\nprint(model.print_trainable_parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:48:59.988942Z","iopub.execute_input":"2025-02-15T20:48:59.989256Z","iopub.status.idle":"2025-02-15T20:49:00.116885Z","shell.execute_reply.started":"2025-02-15T20:48:59.989221Z","shell.execute_reply":"2025-02-15T20:49:00.116218Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,616,384 || all params: 70,972,480 || trainable%: 3.6865\nNone\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Step 4: Prepare Data\ntrain_dataset = TranslationDataset('/kaggle/input/semeval.train./semeval/train/ja/train.jsonl',\n                                   tokenizer)\ndev_dataset = TranslationDataset('/kaggle/input/flattened_validation_dataset_ja.jsonl',\n                                 tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:00.117651Z","iopub.execute_input":"2025-02-15T20:49:00.117885Z","iopub.status.idle":"2025-02-15T20:49:00.241239Z","shell.execute_reply.started":"2025-02-15T20:49:00.117865Z","shell.execute_reply":"2025-02-15T20:49:00.240612Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:00.241967Z","iopub.execute_input":"2025-02-15T20:49:00.242228Z","iopub.status.idle":"2025-02-15T20:49:00.246755Z","shell.execute_reply.started":"2025-02-15T20:49:00.242206Z","shell.execute_reply":"2025-02-15T20:49:00.245966Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"7225"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"len(dev_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:00.247713Z","iopub.execute_input":"2025-02-15T20:49:00.247989Z","iopub.status.idle":"2025-02-15T20:49:00.259881Z","shell.execute_reply.started":"2025-02-15T20:49:00.247960Z","shell.execute_reply":"2025-02-15T20:49:00.259245Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"1409"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ndev_dataloader = DataLoader(dev_dataset, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:00.260748Z","iopub.execute_input":"2025-02-15T20:49:00.261062Z","iopub.status.idle":"2025-02-15T20:49:00.272843Z","shell.execute_reply.started":"2025-02-15T20:49:00.261032Z","shell.execute_reply":"2025-02-15T20:49:00.272168Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback\n\nearly_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=3,  \n    early_stopping_threshold=0.0001  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:00.273635Z","iopub.execute_input":"2025-02-15T20:49:00.274060Z","iopub.status.idle":"2025-02-15T20:49:00.285119Z","shell.execute_reply.started":"2025-02-15T20:49:00.274031Z","shell.execute_reply":"2025-02-15T20:49:00.284426Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:34:18.381586Z","iopub.execute_input":"2025-02-03T16:34:18.381894Z","iopub.status.idle":"2025-02-03T16:34:18.559252Z","shell.execute_reply.started":"2025-02-03T16:34:18.381866Z","shell.execute_reply":"2025-02-03T16:34:18.558224Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# facebook/m2m100_418M","metadata":{}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results/it/opus-mt-en-jap-v1\",\n    report_to=None,\n    evaluation_strategy=\"steps\",\n    logging_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    save_total_limit=1,\n    predict_with_generate=True,\n    load_best_model_at_end = True,\n    metric_for_best_model=\"bleu\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:00.285902Z","iopub.execute_input":"2025-02-15T20:49:00.286167Z","iopub.status.idle":"2025-02-15T20:49:00.449326Z","shell.execute_reply.started":"2025-02-15T20:49:00.286139Z","shell.execute_reply":"2025-02-15T20:49:00.448733Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]  # Ensure labels are lists of lists\n    return preds, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:00.450133Z","iopub.execute_input":"2025-02-15T20:49:00.450341Z","iopub.status.idle":"2025-02-15T20:49:00.454215Z","shell.execute_reply.started":"2025-02-15T20:49:00.450323Z","shell.execute_reply":"2025-02-15T20:49:00.453546Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"bleu = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    result = {k: round(v, 4) for k, v in result.items()}\n\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:00.454974Z","iopub.execute_input":"2025-02-15T20:49:00.455197Z","iopub.status.idle":"2025-02-15T20:49:01.814575Z","shell.execute_reply.started":"2025-02-15T20:49:00.455177Z","shell.execute_reply":"2025-02-15T20:49:01.813769Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3ba24062edd4e14898758ad2aab9319"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=dev_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stopping_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:01.815439Z","iopub.execute_input":"2025-02-15T20:49:01.816079Z","iopub.status.idle":"2025-02-15T20:49:02.257416Z","shell.execute_reply.started":"2025-02-15T20:49:01.816054Z","shell.execute_reply":"2025-02-15T20:49:02.256806Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from unittest import mock\nfrom unittest.mock import Mock\nclass Dummy:\n    def __enter__(self):\n        return Mock()\n\n    def __exit__(self, *args):\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:02.258140Z","iopub.execute_input":"2025-02-15T20:49:02.258367Z","iopub.status.idle":"2025-02-15T20:49:02.262165Z","shell.execute_reply.started":"2025-02-15T20:49:02.258347Z","shell.execute_reply":"2025-02-15T20:49:02.261262Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"with mock.patch('wandb.init', return_value=Dummy()):\n    trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:49:02.264522Z","iopub.execute_input":"2025-02-15T20:49:02.264748Z","iopub.status.idle":"2025-02-15T21:00:38.836707Z","shell.execute_reply.started":"2025-02-15T20:49:02.264728Z","shell.execute_reply":"2025-02-15T21:00:38.835995Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2260 11:34 < 01:30, 2.88 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.372200</td>\n      <td>0.720850</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.951000</td>\n      <td>0.604635</td>\n      <td>0.000000</td>\n      <td>4.011400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.863000</td>\n      <td>0.575122</td>\n      <td>0.000000</td>\n      <td>4.043300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.816400</td>\n      <td>0.575553</td>\n      <td>0.000000</td>\n      <td>4.073100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"model.save_pretrained(\"./trained_model\")\ntokenizer.save_pretrained(\"./trained_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:00:38.837939Z","iopub.execute_input":"2025-02-15T21:00:38.838205Z","iopub.status.idle":"2025-02-15T21:00:39.184539Z","shell.execute_reply.started":"2025-02-15T21:00:38.838182Z","shell.execute_reply":"2025-02-15T21:00:39.183835Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('./trained_model/tokenizer_config.json',\n './trained_model/special_tokens_map.json',\n './trained_model/vocab.json',\n './trained_model/source.spm',\n './trained_model/target.spm',\n './trained_model/added_tokens.json')"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(f\"The Results on Evaluation dataset during fitting are: {eval_results}\")\nprint(f\"Validation BLEU Score: {eval_results['eval_bleu']}\") \n\n# Extract BLEU scores from log history\nbleu_scores = [log['eval_bleu'] for log in trainer.state.log_history if 'eval_bleu' in log]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:00:39.185196Z","iopub.execute_input":"2025-02-15T21:00:39.185406Z","iopub.status.idle":"2025-02-15T21:01:19.143898Z","shell.execute_reply.started":"2025-02-15T21:00:39.185388Z","shell.execute_reply":"2025-02-15T21:01:19.143159Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='89' max='89' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [89/89 00:31]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"The Results on Evaluation dataset during fitting are: {'eval_loss': 0.7208500504493713, 'eval_bleu': 0.0, 'eval_gen_len': 4.0, 'eval_runtime': 39.9509, 'eval_samples_per_second': 35.268, 'eval_steps_per_second': 2.228, 'epoch': 4.424778761061947}\nValidation BLEU Score: 0.0\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(bleu_scores, marker='o', label=\"BLEU Score\")\nplt.xlabel(\"Evaluation Epochs\")\nplt.ylabel(\"BLEU Score\")\nplt.title(\"BLEU Score over Evaluation Epochs\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:19.144709Z","iopub.execute_input":"2025-02-15T21:01:19.144936Z","iopub.status.idle":"2025-02-15T21:01:19.380047Z","shell.execute_reply.started":"2025-02-15T21:01:19.144905Z","shell.execute_reply":"2025-02-15T21:01:19.379368Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, file_path, tokenizer, max_length=128):\n        with open(file_path, 'r', encoding='utf-8') as f:\n            self.data = [json.loads(line) for line in f]\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        source = item['source']\n        inputs = self.tokenizer(\n            source, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt'\n        )\n        return {\n            'id': item['id'],\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze()\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:19.380747Z","iopub.execute_input":"2025-02-15T21:01:19.380945Z","iopub.status.idle":"2025-02-15T21:01:19.385507Z","shell.execute_reply.started":"2025-02-15T21:01:19.380927Z","shell.execute_reply":"2025-02-15T21:01:19.384874Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Load Test Data\ntest_dataset = TestDataset('/kaggle/input/semeval.test_hidden/test_without_targets/ja_JP.jsonl', tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:19.387475Z","iopub.execute_input":"2025-02-15T21:01:19.387691Z","iopub.status.idle":"2025-02-15T21:01:19.463730Z","shell.execute_reply.started":"2025-02-15T21:01:19.387650Z","shell.execute_reply":"2025-02-15T21:01:19.463173Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"test_dataloader = DataLoader(test_dataset, batch_size=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:19.464454Z","iopub.execute_input":"2025-02-15T21:01:19.464643Z","iopub.status.idle":"2025-02-15T21:01:19.467971Z","shell.execute_reply.started":"2025-02-15T21:01:19.464625Z","shell.execute_reply":"2025-02-15T21:01:19.467148Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:19.468790Z","iopub.execute_input":"2025-02-15T21:01:19.469075Z","iopub.status.idle":"2025-02-15T21:01:19.492954Z","shell.execute_reply.started":"2025-02-15T21:01:19.469035Z","shell.execute_reply":"2025-02-15T21:01:19.492175Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): MarianMTModel(\n      (model): MarianModel(\n        (shared): Embedding(46276, 512, padding_idx=46275)\n        (encoder): MarianEncoder(\n          (embed_tokens): Embedding(46276, 512, padding_idx=46275)\n          (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n          (layers): ModuleList(\n            (0-5): 6 x MarianEncoderLayer(\n              (self_attn): MarianAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (activation_fn): SiLU()\n              (fc1): lora.Linear(\n                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=512, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (fc2): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (decoder): MarianDecoder(\n          (embed_tokens): Embedding(46276, 512, padding_idx=46275)\n          (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n          (layers): ModuleList(\n            (0-5): 6 x MarianDecoderLayer(\n              (self_attn): MarianAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (activation_fn): SiLU()\n              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (encoder_attn): MarianAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fc1): lora.Linear(\n                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=512, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (fc2): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=512, out_features=46276, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.1, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=512, out_features=16, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=16, out_features=46276, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n        (lora_magnitude_vector): ModuleDict()\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T06:37:46.458316Z","iopub.execute_input":"2025-02-05T06:37:46.458628Z","iopub.status.idle":"2025-02-05T06:37:46.463677Z","shell.execute_reply.started":"2025-02-05T06:37:46.458600Z","shell.execute_reply":"2025-02-05T06:37:46.462965Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:19.493608Z","iopub.execute_input":"2025-02-15T21:01:19.493822Z","iopub.status.idle":"2025-02-15T21:01:19.506341Z","shell.execute_reply.started":"2025-02-15T21:01:19.493803Z","shell.execute_reply":"2025-02-15T21:01:19.505525Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): MarianMTModel(\n      (model): MarianModel(\n        (shared): Embedding(46276, 512, padding_idx=46275)\n        (encoder): MarianEncoder(\n          (embed_tokens): Embedding(46276, 512, padding_idx=46275)\n          (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n          (layers): ModuleList(\n            (0-5): 6 x MarianEncoderLayer(\n              (self_attn): MarianAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (activation_fn): SiLU()\n              (fc1): lora.Linear(\n                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=512, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (fc2): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (decoder): MarianDecoder(\n          (embed_tokens): Embedding(46276, 512, padding_idx=46275)\n          (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n          (layers): ModuleList(\n            (0-5): 6 x MarianDecoderLayer(\n              (self_attn): MarianAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (activation_fn): SiLU()\n              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (encoder_attn): MarianAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fc1): lora.Linear(\n                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=512, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (fc2): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=512, out_features=46276, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.1, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=512, out_features=16, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=16, out_features=46276, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n        (lora_magnitude_vector): ModuleDict()\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"test_jsonl_path = \"/kaggle/input/semeval.test_hidden/test_without_targets/ja_JP.jsonl\"\nsource_texts = {}\n\nwith open(test_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        data = json.loads(line)\n        source_texts[data[\"id\"]] = data[\"source\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:19.507177Z","iopub.execute_input":"2025-02-15T21:01:19.507406Z","iopub.status.idle":"2025-02-15T21:01:19.545152Z","shell.execute_reply.started":"2025-02-15T21:01:19.507375Z","shell.execute_reply":"2025-02-15T21:01:19.544335Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"predictions = []\n\nfor batch in tqdm(test_dataloader, desc=\"Generating Predictions\", colour=\"blue\"):\n    with torch.no_grad():\n        generated_ids = model.generate(\n            input_ids=batch[\"input_ids\"].to(model.device),\n            attention_mask=batch[\"attention_mask\"].to(model.device),\n            max_length=128\n        )\n        prediction = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n        batch_id = batch[\"id\"][0]  \n        source_text = source_texts[batch_id]\n\n        # print(f\"The source text is: {source_text}\")\n        # print(f\"The prediction text is: {prediction}\")\n        \n        predictions.append({\n            \"id\": batch_id,\n            \"source_language\": \"English\",\n            \"target_language\": \"Italian\",\n            \"text\": source_text,\n            \"prediction\": prediction\n        })\n        # print(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:19.545890Z","iopub.execute_input":"2025-02-15T21:01:19.546145Z","iopub.status.idle":"2025-02-15T21:08:54.692201Z","shell.execute_reply.started":"2025-02-15T21:01:19.546125Z","shell.execute_reply":"2025-02-15T21:08:54.691456Z"}},"outputs":[{"name":"stderr","text":"Generating Predictions: 100%|\u001b[34m██████████\u001b[0m| 5108/5108 [07:35<00:00, 11.22it/s]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"predictions[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:43:50.052219Z","iopub.execute_input":"2025-02-15T20:43:50.052551Z","iopub.status.idle":"2025-02-15T20:43:50.058241Z","shell.execute_reply.started":"2025-02-15T20:43:50.052518Z","shell.execute_reply":"2025-02-15T20:43:50.057417Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[{'id': 'e033d5afc49c9c27',\n  'source_language': 'English',\n  'target_language': 'Italian',\n  'text': 'What is the significance of Šarena Mosque in Tetovo, North Macedonia?',\n  'prediction': 'Qual è il masa di Šarena a Tetovo, Macedonia del Nord?'},\n {'id': '57b14b52b7758492',\n  'source_language': 'English',\n  'target_language': 'Italian',\n  'text': 'How would you describe the architectural style of Šarena Mosque?',\n  'prediction': 'Qual è la descrivizione dello stolo atblicico della mosque di Šarena?'},\n {'id': 'dfdbdd81c476c5e2',\n  'source_language': 'English',\n  'target_language': 'Italian',\n  'text': 'In which city is Šarena Mosque located?',\n  'prediction': 'In quale città si trova la moschea di Šarena?'},\n {'id': '00bcab05e9ecd57b',\n  'source_language': 'English',\n  'target_language': 'Italian',\n  'text': 'Where is the Basilica of St. Sernin situated?',\n  'prediction': 'Dove si trova la Basilica di S. Serin?'},\n {'id': '99fd605868d203e8',\n  'source_language': 'English',\n  'target_language': 'Italian',\n  'text': 'Where is the Basilica of St. Sernin located?',\n  'prediction': 'Dove si trova la Basilica di S. Serin?'}]"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"len(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:10:48.815265Z","iopub.execute_input":"2025-02-15T21:10:48.815637Z","iopub.status.idle":"2025-02-15T21:10:48.820610Z","shell.execute_reply.started":"2025-02-15T21:10:48.815606Z","shell.execute_reply":"2025-02-15T21:10:48.819880Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"5108"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Save Predictions\nwith open('ar_AE_opus-mt-en-ja-v1.jsonl', 'w') as f:\n    for pred in predictions:\n        f.write(json.dumps(pred, ensure_ascii=False) + '\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:10:51.460184Z","iopub.execute_input":"2025-02-15T21:10:51.460481Z","iopub.status.idle":"2025-02-15T21:10:51.494091Z","shell.execute_reply.started":"2025-02-15T21:10:51.460458Z","shell.execute_reply":"2025-02-15T21:10:51.493494Z"}},"outputs":[],"execution_count":34}]}